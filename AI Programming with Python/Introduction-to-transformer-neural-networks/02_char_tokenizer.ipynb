{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SwimLane2/udacity-ai-masters/blob/main/AI%20Programming%20with%20Python/Introduction-to-transformer-neural-networks/02_char_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z2ea-5SaF59N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Use CUDA if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9CRij9k1GDlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90df1316-1ed9-449d-97f1-71cbd4e5cc57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DSc0ludHGE1o"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "text = Path('/content/tiny-shakespeare.txt').read_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7-9Nk7OoGGHc",
        "outputId": "a28ced48-d135-46fe-fd02-01bc57007c53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[0:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qg-yHMXPGHYq"
      },
      "outputs": [],
      "source": [
        "class CharTokenizer:\n",
        "    def __init__(self, vocabulary):\n",
        "        self.token_id_for_char = {\n",
        "            char: token_id for token_id, char in enumerate(vocabulary)\n",
        "        }\n",
        "        self.char_for_token_id = {\n",
        "            token_id: char for token_id, char in enumerate(vocabulary)\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def train_from_text(text):\n",
        "        vocabulary = set(text)\n",
        "        return CharTokenizer(sorted(list(vocabulary)))\n",
        "\n",
        "    def encode(self, text):\n",
        "        token_ids = []\n",
        "        for char in text:\n",
        "            token_ids.append(self.token_id_for_char[char])\n",
        "        return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        chars = []\n",
        "        for token_id in token_ids.tolist():\n",
        "            chars.append(self.char_for_token_id[token_id])\n",
        "        return \"\".join(chars)\n",
        "\n",
        "    def vocabulary_size(self):\n",
        "        return len(self.token_id_for_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pAYBFds4GNAb"
      },
      "outputs": [],
      "source": [
        "tokenizer = CharTokenizer.train_from_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LKq-R9xvJ3RX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8029d1a3-f024-4553-d8e4-f0694a685c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([20, 43, 50, 50, 53,  1, 61, 53, 56, 50, 42])\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.encode(\"Hello world\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tWpR_hr9GOKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8998c044-7c7a-4310-85b1-08e970e981da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3a7qPM-mGPur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c824d60f-c588-4eb3-dbec-67fabe952c1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokenizer.vocabulary_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UbtX7_JtHFXL"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(depth=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2I3m9KI6HM-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7faa5df-410e-441f-b74e-7cb730f6844a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '\\n',\n",
            " 1: ' ',\n",
            " 2: '!',\n",
            " 3: '$',\n",
            " 4: '&',\n",
            " 5: \"'\",\n",
            " 6: ',',\n",
            " 7: '-',\n",
            " 8: '.',\n",
            " 9: '3',\n",
            " 10: ':',\n",
            " 11: ';',\n",
            " 12: '?',\n",
            " 13: 'A',\n",
            " 14: 'B',\n",
            " 15: 'C',\n",
            " 16: 'D',\n",
            " 17: 'E',\n",
            " 18: 'F',\n",
            " 19: 'G',\n",
            " 20: 'H',\n",
            " 21: 'I',\n",
            " 22: 'J',\n",
            " 23: 'K',\n",
            " 24: 'L',\n",
            " 25: 'M',\n",
            " 26: 'N',\n",
            " 27: 'O',\n",
            " 28: 'P',\n",
            " 29: 'Q',\n",
            " 30: 'R',\n",
            " 31: 'S',\n",
            " 32: 'T',\n",
            " 33: 'U',\n",
            " 34: 'V',\n",
            " 35: 'W',\n",
            " 36: 'X',\n",
            " 37: 'Y',\n",
            " 38: 'Z',\n",
            " 39: 'a',\n",
            " 40: 'b',\n",
            " 41: 'c',\n",
            " 42: 'd',\n",
            " 43: 'e',\n",
            " 44: 'f',\n",
            " 45: 'g',\n",
            " 46: 'h',\n",
            " 47: 'i',\n",
            " 48: 'j',\n",
            " 49: 'k',\n",
            " 50: 'l',\n",
            " 51: 'm',\n",
            " 52: 'n',\n",
            " 53: 'o',\n",
            " 54: 'p',\n",
            " 55: 'q',\n",
            " 56: 'r',\n",
            " 57: 's',\n",
            " 58: 't',\n",
            " 59: 'u',\n",
            " 60: 'v',\n",
            " 61: 'w',\n",
            " 62: 'x',\n",
            " 63: 'y',\n",
            " 64: 'z'}\n"
          ]
        }
      ],
      "source": [
        "pp.pprint(tokenizer.char_for_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "olX9rHjxHQih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f84ceae-ea32-4b7a-9a1e-4043ce052b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0,\n",
            " ' ': 1,\n",
            " '!': 2,\n",
            " '$': 3,\n",
            " '&': 4,\n",
            " \"'\": 5,\n",
            " ',': 6,\n",
            " '-': 7,\n",
            " '.': 8,\n",
            " '3': 9,\n",
            " ':': 10,\n",
            " ';': 11,\n",
            " '?': 12,\n",
            " 'A': 13,\n",
            " 'B': 14,\n",
            " 'C': 15,\n",
            " 'D': 16,\n",
            " 'E': 17,\n",
            " 'F': 18,\n",
            " 'G': 19,\n",
            " 'H': 20,\n",
            " 'I': 21,\n",
            " 'J': 22,\n",
            " 'K': 23,\n",
            " 'L': 24,\n",
            " 'M': 25,\n",
            " 'N': 26,\n",
            " 'O': 27,\n",
            " 'P': 28,\n",
            " 'Q': 29,\n",
            " 'R': 30,\n",
            " 'S': 31,\n",
            " 'T': 32,\n",
            " 'U': 33,\n",
            " 'V': 34,\n",
            " 'W': 35,\n",
            " 'X': 36,\n",
            " 'Y': 37,\n",
            " 'Z': 38,\n",
            " 'a': 39,\n",
            " 'b': 40,\n",
            " 'c': 41,\n",
            " 'd': 42,\n",
            " 'e': 43,\n",
            " 'f': 44,\n",
            " 'g': 45,\n",
            " 'h': 46,\n",
            " 'i': 47,\n",
            " 'j': 48,\n",
            " 'k': 49,\n",
            " 'l': 50,\n",
            " 'm': 51,\n",
            " 'n': 52,\n",
            " 'o': 53,\n",
            " 'p': 54,\n",
            " 'q': 55,\n",
            " 'r': 56,\n",
            " 's': 57,\n",
            " 't': 58,\n",
            " 'u': 59,\n",
            " 'v': 60,\n",
            " 'w': 61,\n",
            " 'x': 62,\n",
            " 'y': 63,\n",
            " 'z': 64}\n"
          ]
        }
      ],
      "source": [
        "pp.pprint(tokenizer.token_id_for_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NLnPsybaHTew"
      },
      "outputs": [],
      "source": [
        "# Step 1 - Define the `TokenIdsDataset` Class\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TokenIdsDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        # TODO: Save data and block size\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: If every position can be a start of an item,\n",
        "        # and all items should be \"block_size\", compute the size\n",
        "        # of the dataset\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, pos):\n",
        "        # TODO: Check if the input position is valid\n",
        "        assert pos < len(self.data) - self.block_size\n",
        "\n",
        "        # TODO: Get an item from position \"pos\"\n",
        "        # TODO: Get a target item (shifted by one position)\n",
        "\n",
        "        x = self.data[pos:pos + self.block_size]\n",
        "        y = self.data[pos + 1:pos + 1 + self.block_size]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "        # TODO: Return both"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 - Tokenize the Text\n",
        "\n",
        "# TODO: Encode text using the tokenizer\n",
        "# Create \"TokenIdsDataset\" with the tokenized text, and block_size=64\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "dataset = TokenIdsDataset(tokenized_text, block_size=64)"
      ],
      "metadata": {
        "id": "hnuDLYNX3HNR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 - Retrieve the First Item from the Dataset\n",
        "\n",
        "# TODO: Get the first item from the dataset\n",
        "# Decode \"x\" using tokenizer.decode\n",
        "\n",
        "x, y = dataset[0]\n",
        "print(tokenizer.decode(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrJ8hhbK3WN5",
        "outputId": "f8f53dc3-40d2-496b-ecb0-4acccaa4497e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "Al\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "# RandomSampler allows to read random items from a datasset\n",
        "sampler = RandomSampler(dataset, replacement=True)\n",
        "# Dataloader will laod two random samplers using the sampler\n",
        "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler)"
      ],
      "metadata": {
        "id": "EizOqQHl3lyN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4 - Use a DataLoader\n",
        "\n",
        "# TODO: Get a single batch from the \"dataloader\"\n",
        "# For this call the `iter` function, and pass DataLoader instance to it. This will create an iterator\n",
        "# Then call the `next` function and pass the iterator to it to get the first training batch\n",
        "\n",
        "x, y = next(iter(dataloader))"
      ],
      "metadata": {
        "id": "HpBCdhTv3nHw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8atYwhs87JEH",
        "outputId": "95936835-2364-44b8-ffbc-155536abb813"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Decode input item\n",
        "print(tokenizer.decode(x[0]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmU0qFy93xkI",
        "outputId": "eb2da9a9-6df2-4fed-e23e-f23da3da6fd9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the signal to begin.\n",
            "\n",
            "Lord Marshal:\n",
            "Sound, trumpets; and set fo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Decode target item\n",
        "print(tokenizer.decode(y[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPhmDk-S33jM",
        "outputId": "1e28aaf8-1ddd-447e-c137-bc032eb2a825"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the signal to begin.\n",
            "\n",
            "Lord Marshal:\n",
            "Sound, trumpets; and set for\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "ai-programming-in-python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}